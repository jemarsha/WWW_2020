{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jermainemarshall/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "embed_dim=100\n",
    "G=4\n",
    "#batch_size=80\n",
    "number_of_negative_tokens = 5\n",
    "n_epochs = 30\n",
    "n_workers=64\n",
    "queue_size = 64\n",
    "padding_length=3\n",
    "title_length_percentile=95\n",
    "desc_length_percentile=90\n",
    "\n",
    "import os, time, json,pandas as pd, numpy as np, re, string, random,sys\n",
    "from collections import Counter,defaultdict\n",
    "import Levenshtein\n",
    "\n",
    "\n",
    "from keras.layers import Input,Lambda,Conv1D,GlobalMaxPool1D,concatenate,Dense,Dropout,PReLU,TimeDistributed,Embedding,Activation\n",
    "from keras import Model\n",
    "import keras.backend as K\n",
    "import keras\n",
    "from keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "df= pd.read_csv('allrecipes.csv')\n",
    "\n",
    "\n",
    "# In[7]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    12752\n",
       "2    10950\n",
       "3     6194\n",
       "4     3017\n",
       "0     1600\n",
       "5     1412\n",
       "6      444\n",
       "7       60\n",
       "Name: WHO, dtype: int64"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.WHO.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RECIPE</th>\n",
       "      <th>WHO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mayonnaise#237.0,parmigiano reggiano cheese#75.0,granulated sugar#25.200000000000003,romaine lettuce#283.0,bacon bits#56.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                       RECIPE  \\\n",
       "0  mayonnaise#237.0,parmigiano reggiano cheese#75.0,granulated sugar#25.200000000000003,romaine lettuce#283.0,bacon bits#56.0   \n",
       "\n",
       "   WHO  \n",
       "0    1  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth= 500\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install python-levenshtein\n",
    "#!pip3 install setuptools\n",
    "\n",
    "\n",
    "# strip punctuation and combine ingredients into one word separated by a spae\n",
    "\n",
    "#df['RECIPE'] = df['RECIPE'].str.replace('\\d+', '')\n",
    "\n",
    "#df['RECIPE'] = df['RECIPE'].str.replace(' ', '')\n",
    "df['RECIPE'] = df['RECIPE'].str.replace(' ', '')\n",
    "df['RECIPE'] = df['RECIPE'].str.replace('#', ' ')\n",
    "#df['RECIPE'] = df['RECIPE'].str.replace(r'[\\s^\\w]', '')\n",
    "#df['RECIPE'] = df['RECIPE'].str.replace(' ', '')\n",
    "df['RECIPE'] = df['RECIPE'].str.replace(',', ' ')\n",
    "#df['RECIPE'] = df['RECIPE'].str.replace('\\d', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RECIPE</th>\n",
       "      <th>WHO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mayonnaise 237.0 parmigianoreggianocheese 75.0 granulatedsugar 25.200000000000003 romainelettuce 283.0 baconbits 56.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PhiladelphiaCreamCheese 454.0 fineseasalt 3.0 pecanhalves 164.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hardboiledeggs 100.0 wholemilk 231.31199999999998 cakeflour 83.0 fineseasalt 0.4 vegetableoil 6.8</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zucchini 646.0 extra-virginoliveoil 13.5 hotwater 58.21333333333333 fineseasalt 0.4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>chiliflakes 1.7999999999999998 fineseasalt 0.4 honey 339.0 saltedbutter 114.0 srirachasauce 153.4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>granulatedsugar 403.0 cinnamon 2.6 gratednutmeg 2.2 fineseasalt 0.4 saltedbutter 114.0 purevanillaextract 12.600000000000001</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>finelychoppedonion 150.0 fineseasalt 12.0 chiliflakes 1.7999999999999998 largegarliccloves 2.72</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>orangejuice 117.056 limewedges 33.5 honey 20.34 chiliflakes 1.7999999999999998 chickenbreasts 1536.0 coriander 0.24</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>honey 61.019999999999996 lowsodiumsoysauce 45.0 balsamicvinegar 48.0 largegarliccloves 8.16 freshlygroundpepper 2.3</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>garlicpowder 9.3 chilipowder 8.100000000000001 vegetableoil 13.6</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>leangroundbeef 910.0 finelychoppedonion 150.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>shortening 308.0 granulatedsugar 600.0 hardboiledeggs 300.0 wholemilk 231.31199999999998 cakeflour 438.0 fineseasalt 6.0 purevanillaextract 4.2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                             RECIPE  \\\n",
       "0                             mayonnaise 237.0 parmigianoreggianocheese 75.0 granulatedsugar 25.200000000000003 romainelettuce 283.0 baconbits 56.0   \n",
       "1                                                                                   PhiladelphiaCreamCheese 454.0 fineseasalt 3.0 pecanhalves 164.0   \n",
       "2                                                 hardboiledeggs 100.0 wholemilk 231.31199999999998 cakeflour 83.0 fineseasalt 0.4 vegetableoil 6.8   \n",
       "3                                                               zucchini 646.0 extra-virginoliveoil 13.5 hotwater 58.21333333333333 fineseasalt 0.4   \n",
       "4                                                 chiliflakes 1.7999999999999998 fineseasalt 0.4 honey 339.0 saltedbutter 114.0 srirachasauce 153.4   \n",
       "5                      granulatedsugar 403.0 cinnamon 2.6 gratednutmeg 2.2 fineseasalt 0.4 saltedbutter 114.0 purevanillaextract 12.600000000000001   \n",
       "6                                                   finelychoppedonion 150.0 fineseasalt 12.0 chiliflakes 1.7999999999999998 largegarliccloves 2.72   \n",
       "7                               orangejuice 117.056 limewedges 33.5 honey 20.34 chiliflakes 1.7999999999999998 chickenbreasts 1536.0 coriander 0.24   \n",
       "8                               honey 61.019999999999996 lowsodiumsoysauce 45.0 balsamicvinegar 48.0 largegarliccloves 8.16 freshlygroundpepper 2.3   \n",
       "9                                                                                  garlicpowder 9.3 chilipowder 8.100000000000001 vegetableoil 13.6   \n",
       "10                                                                                                    leangroundbeef 910.0 finelychoppedonion 150.0   \n",
       "11  shortening 308.0 granulatedsugar 600.0 hardboiledeggs 300.0 wholemilk 231.31199999999998 cakeflour 438.0 fineseasalt 6.0 purevanillaextract 4.2   \n",
       "\n",
       "    WHO  \n",
       "0     1  \n",
       "1     1  \n",
       "2     3  \n",
       "3     3  \n",
       "4     0  \n",
       "5     3  \n",
       "6     2  \n",
       "7     3  \n",
       "8     2  \n",
       "9     4  \n",
       "10    2  \n",
       "11    1  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WHO'] = df['WHO'].replace({0:'zero', 1: 'one', 2: 'two', 3: 'three', 4: 'four', 5: 'five', 6: 'six', 7: 'seven'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RECIPE</th>\n",
       "      <th>WHO</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mayonnaise 237.0 parmigianoreggianocheese 75.0 granulatedsugar 25.200000000000003 romainelettuce 283.0 baconbits 56.0</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PhiladelphiaCreamCheese 454.0 fineseasalt 3.0 pecanhalves 164.0</td>\n",
       "      <td>one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hardboiledeggs 100.0 wholemilk 231.31199999999998 cakeflour 83.0 fineseasalt 0.4 vegetableoil 6.8</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zucchini 646.0 extra-virginoliveoil 13.5 hotwater 58.21333333333333 fineseasalt 0.4</td>\n",
       "      <td>three</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                  RECIPE  \\\n",
       "0  mayonnaise 237.0 parmigianoreggianocheese 75.0 granulatedsugar 25.200000000000003 romainelettuce 283.0 baconbits 56.0   \n",
       "1                                                        PhiladelphiaCreamCheese 454.0 fineseasalt 3.0 pecanhalves 164.0   \n",
       "2                      hardboiledeggs 100.0 wholemilk 231.31199999999998 cakeflour 83.0 fineseasalt 0.4 vegetableoil 6.8   \n",
       "3                                    zucchini 646.0 extra-virginoliveoil 13.5 hotwater 58.21333333333333 fineseasalt 0.4   \n",
       "\n",
       "     WHO  \n",
       "0    one  \n",
       "1    one  \n",
       "2  three  \n",
       "3  three  "
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_colwidth= 500\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#(X_train, X_test) = train_test_split(df2,test_size=0.20)\n",
    "\n",
    "\n",
    "#with tf.device('/gpu:0'):\n",
    "X_train, X_validation, X_test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
    "\n",
    "descTokenCounts = pd.Series(' '.join(df['RECIPE']).split())\n",
    "\n",
    "descTokenCounts= descTokenCounts.value_counts().to_dict()\n",
    "\n",
    "df= df.drop_duplicates()\n",
    "\n",
    "\n",
    "df= df.dropna()\n",
    "\n",
    "\n",
    "def remove_non_ascii_1(text):\n",
    "\n",
    "    return ''.join([i if ord(i) < 128 else ' ' for i in text])\n",
    "\n",
    "def remove_non_ascii_2(text):\n",
    "\n",
    "    return re.sub(r'[^\\x20-\\x7E]+',' ', text)\n",
    "\n",
    "vocabSize=100000\n",
    "tokens = sorted(descTokenCounts.items(),key=lambda x:x[1],reverse=True)[:vocabSize]\n",
    "\n",
    "tokens = [i[0] for i in tokens]\n",
    "tokens.append('token_unknown')\n",
    "token_to_index = {}\n",
    "index_to_token = {}\n",
    "\n",
    "\n",
    "df['WHO'] = ([remove_non_ascii_1(sentence) for sentence in df['WHO']])\n",
    "df['WHO'] = ([remove_non_ascii_2(sentence) for sentence in df['WHO']])\n",
    "\n",
    "\n",
    "df['RECIPE']= ([remove_non_ascii_1(sentence) for sentence in df['RECIPE']])\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['RECIPE']= ([remove_non_ascii_2(sentence) for sentence in df['RECIPE']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'allmy_jermaine_model1_jerm'\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#(X_train, X_test) = train_test_split(df2,test_size=0.20)\n",
    "\n",
    "\n",
    "#with tf.device('/gpu:0'):\n",
    "X_train, X_validation, X_test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
    "\n",
    "\n",
    "\n",
    "with open('alltrain_data.json', 'wb') as f:\n",
    "    f.write(X_train.to_json(orient='records', lines=True).encode('ascii', 'ignore'))\n",
    "\n",
    "    \n",
    "with open('alltest_data.json', 'wb') as g:\n",
    "    g.write(X_test.to_json(orient='records', lines=True).encode('ascii', 'ignore'))\n",
    "\n",
    "\n",
    "    \n",
    "with open('allvalidation_data.json', 'wb') as t:\n",
    "    t.write(X_validation.to_json(orient='records', lines=True).encode('ascii', 'ignore'))\n",
    "\n",
    "def loadData(dataPath):\n",
    "    data=[]\n",
    "    start_time = time.time()\n",
    "    with open(dataPath) as f:\n",
    "        for line in f:\n",
    "            data.append(line)\n",
    "    loadTime = time.time() - start_time\n",
    "\n",
    "    #print loadTime\n",
    "    return data\n",
    "\n",
    "training_data = loadData('alltrain_data.json')\n",
    "validation_data = loadData('allvalidation_data.json') \n",
    "test_data = loadData('alltest_data.json') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_length= df['WHO'].map(len)\n",
    "desc_length= df['RECIPE'].map(len)\n",
    "titleCounts = df['WHO'].value_counts().to_dict()\n",
    "\n",
    "charset=set(list(string.ascii_lowercase)+list(map(str,range(10)))+list(\"?$*!: @ ^ %- / <> ~ ` \\\\ [] {} \\n = | \\\" _+#&-/\\n %() ' '.;,\"))\n",
    "title_char_to_index_dict={c:i for i,c in enumerate(charset)}\n",
    "title_index_to_char_dict={i:c for c,i in title_char_to_index_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'z': 0, 'x': 1, 'o': 2, ',': 3, 's': 4, 'w': 5, '5': 6, '6': 7, '<': 8, ')': 9, '.': 10, 'u': 11, '}': 12, '=': 13, 'c': 14, '>': 15, 't': 16, '\\\\': 17, 'd': 18, '3': 19, 'p': 20, '7': 21, 'm': 22, '9': 23, 'q': 24, '/': 25, '#': 26, '&': 27, '_': 28, '(': 29, ' ': 30, ';': 31, 'e': 32, '^': 33, '|': 34, '~': 35, '!': 36, '+': 37, 'k': 38, '[': 39, 'f': 40, 'i': 41, 'l': 42, '8': 43, '?': 44, '%': 45, '2': 46, '*': 47, 'r': 48, 'y': 49, 'n': 50, ':': 51, 'v': 52, '-': 53, '{': 54, '1': 55, 'h': 56, '`': 57, 'a': 58, 'j': 59, '0': 60, ']': 61, '\"': 62, \"'\": 63, 'b': 64, 'g': 65, '4': 66, '\\n': 67, '$': 68, '@': 69}\n"
     ]
    }
   ],
   "source": [
    "print(title_char_to_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    token_to_index[tokens[i]] = i+1\n",
    "    index_to_token[i+1] = tokens[i] \n",
    "\n",
    "\n",
    "with open('allmodels_indices.json'.format(model_name),'w') as f:\n",
    "    json.dump({'title_char_to_index_dict':title_char_to_index_dict,'title_index_to_char_dict':title_index_to_char_dict,              'token_to_index':token_to_index,'index_to_token':index_to_token},f)\n",
    "\n",
    "\n",
    "    \n",
    "title_max_len = np.ceil(np.percentile(title_length, title_length_percentile)).astype(int)\n",
    "\n",
    "def title_char_tokenizer(entity,entity_max_len):\n",
    "    preproced_entity=(\" \"*padding_length+entity)[:entity_max_len]\n",
    "    padded_entity=preproced_entity+\"\".join([\" \"]*(entity_max_len-len(preproced_entity)))\n",
    "    return [title_char_to_index_dict[i] for i in padded_entity]\n",
    "\n",
    "\n",
    "## RECIPE tokenizer\n",
    "## 0 padded in the end\n",
    "desc_max_length = np.ceil(np.percentile(desc_length, desc_length_percentile)).astype(int)\n",
    "def job_description_tokenizer(desc,entity_max_len):\n",
    "    desc = re.sub(\"[,\\/]\",' ',desc).split()\n",
    "    desc = [token_to_index[i] if i in token_to_index else token_to_index['token_unknown'] for i in desc]\n",
    "    desc=(desc+[0]*entity_max_len)[:entity_max_len]\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('allallmodels_model_params.json'.format(model_name),'w') as f:\n",
    "    json.dump({'title_max_len':int(title_max_len),'desc_max_length':int(desc_max_length)},f)\n",
    "    \n",
    "## character embedding model for a list of titles\n",
    "def char_embed_model_seq(input_layer,char_length,char_index_dict,entity_name,n_filters=512,embed_dim=embed_dim):\n",
    "    '''\n",
    "    Same as char_embed_model, but produces embeddings for a sequence of say, titles.\n",
    "    Applies the same layers (as char_embed_model) to each member of the sequence\n",
    "    '''    \n",
    "    entity_onehot_layer=TimeDistributed(Lambda(lambda x: K.one_hot(x,len(char_index_dict)),output_shape=(char_length,len(char_index_dict))),name=entity_name+'_onehot_seq')(input_layer)\n",
    "        \n",
    "    conv7_layer=TimeDistributed(Conv1D(filters=n_filters,kernel_size=4,activation='relu'),name=entity_name+'_conv_7seq')(entity_onehot_layer)\n",
    "    maxpool7_layer=TimeDistributed(GlobalMaxPool1D(),name=entity_name+'maxpool1seq')(conv7_layer)\n",
    "\n",
    "    conv3_layer=TimeDistributed(Conv1D(filters=n_filters,kernel_size=2,activation='relu'),name=entity_name+'_conv_3seq')(entity_onehot_layer)\n",
    "    maxpool3_layer=TimeDistributed(GlobalMaxPool1D(),name=entity_name+'maxpool2seq')(conv3_layer)\n",
    "\n",
    "    pools_concat=keras.layers.concatenate([maxpool7_layer,maxpool3_layer],name=entity_name+'_maxpools_concatseq')\n",
    "    #pools_dropped=(pools_concat)\n",
    "\n",
    "    entity_dense_layer=TimeDistributed(Dense(n_filters*2,activation='relu'),name=entity_name+'_intermediate_denseseq')(pools_concat)\n",
    "    \n",
    "    dropout_layer=TimeDistributed(Dropout(0.4),name=entity_name+\"_droppedseq\")(entity_dense_layer)\n",
    "    \n",
    "    entity_final_dense=TimeDistributed(Dense(embed_dim),name=entity_name+'_last_denseseq')(dropout_layer)\n",
    "    entity_prelu_layer=TimeDistributed(PReLU(),name=entity_name+'_preluseq')(entity_final_dense)\n",
    "    embed_norm_layer=TimeDistributed(Lambda(lambda x: x/(K.epsilon()+K.sqrt(K.batch_dot(x,x,axes=-1)))),name=entity_name+'_unit_norm_embedseq')(entity_prelu_layer)\n",
    "\n",
    "    entity_embed_model=keras.models.Model(input_layer,embed_norm_layer,name=entity_name+\"_embedding_model\")\n",
    "    return entity_embed_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "## job describtion embedding - average word embedding (remove 0 padding)\n",
    "def desc_embed_model(input_layer,vocab_size,entity_name,embed_dim=embed_dim):\n",
    "    wordEmbed = Embedding(input_dim = vocab_size,output_dim = embed_dim, name='word_embedding')(input_layer) \n",
    "    \n",
    "    ## get average embedding for desc (remove 0 padding)\n",
    "    mask=Lambda(lambda x: K.cast(K.minimum(1,K.cast(x,'int32')),'float32'),name='mask')(input_layer)\n",
    "    mask_summed=Lambda(lambda x: 1/(K.epsilon()+K.sum(x,axis=-1)),name='mask_summed')(mask)\n",
    "    mask_normed=keras.layers.multiply([mask,mask_summed],name='mask_normed')\n",
    "    desc_embedding_averaged=keras.layers.dot([wordEmbed,mask_normed],axes=1,name='desc_embedding_averaged')\n",
    "    \n",
    "    desc_embed_model=Model(input_layer,desc_embedding_averaged,name=entity_name+\"_embedding_model\")\n",
    "    return desc_embed_model\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "alpha=0.5\n",
    "sampling_titles = titleCounts.keys()\n",
    "\n",
    "sampling_titles= [k for k in titleCounts.keys()]\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "sampling_titles_array = np.array(sampling_titles)\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "sampling_prob_array = np.array(list(titleCounts.values()))\n",
    "sampling_prob_array = np.power(sampling_prob_array,alpha)\n",
    "sampling_prob_array = sampling_prob_array/sum(sampling_prob_array)\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "def sampling_based_on_similarity(title,sub_sampling_titles,number_of_negative_tokens=5):\n",
    "## Randomly sample sub_sampling_titles based on emperical distribution\n",
    "## Randomly sample titles from the selected sub-titles based on Levenshtein similarity\n",
    "    st = time.time()\n",
    "    sampling_titles = np.array(sub_sampling_titles)\n",
    "\n",
    "    sampling_prob = np.array([Levenshtein.ratio(title,i) for i in sub_sampling_titles])#Levenshtein.ratio(title,i) for i in sub_sampling_titles])\n",
    "    sampling_prob = sampling_prob/sum(sampling_prob)\n",
    "    \n",
    "    samples=np.random.choice(sampling_titles,size=number_of_negative_tokens,p=sampling_prob)\n",
    "  \n",
    "    return samples.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class empirical_sampling_gen(Sequence):\n",
    "\n",
    "    def __init__(self, data, batch_size,number_of_negative_tokens,sampling_titles,sampling_prob,sub_sampling_size):\n",
    "        '''        \n",
    "        batch_size: number of resumes to process per batch        \n",
    "        The actual number of training samples varies as each resume is splitted into sub-sequences\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        self.input_data = data\n",
    "        self.data_length = len(data)\n",
    "        self.number_negative_tokens = number_of_negative_tokens\n",
    "        self.sampling_titles = sampling_titles\n",
    "        self.sampling_prob = sampling_prob\n",
    "        self.sub_sampling_size = sub_sampling_size\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.data_length / float(self.batch_size)))\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = idx\n",
    "        batch_size = self.batch_size \n",
    "        desc_input_batch=[]\n",
    "        title_input_batch=[]\n",
    "        y_pos_batch=[]\n",
    "        y_neg_batch=[] \n",
    "        titles = []\n",
    "        for k in range(i * batch_size, min((i + 1) * batch_size, self.data_length)):\n",
    "            line = json.loads(self.input_data[k])\n",
    "            desc = job_description_tokenizer(line['RECIPE'],desc_max_length)\n",
    "            #desc = str(desc)\n",
    "            titles.append(line['WHO'])\n",
    "            title = title_char_tokenizer(line['WHO'],title_max_len)\n",
    "            #title= str(title)\n",
    "            desc_input_batch.append(desc)\n",
    "            title_input_batch.append([title]*self.number_negative_tokens)\n",
    "            y_pos_batch.append(1)\n",
    "            y_neg_batch.append([0]*self.number_negative_tokens)  \n",
    "            \n",
    "        ## Sample negative titles for each batch all together              \n",
    "        neg_titles = np.random.choice(self.sampling_titles,                                                 size=len(titles)*self.number_negative_tokens,                                                 p=self.sampling_prob)\n",
    "        \n",
    "        ## Resample for titles with length below median title length based on Levenshtein similarity\n",
    "        sub_sampling_titles = np.random.choice(self.sampling_titles,p=self.sampling_prob,                                               size=self.sub_sampling_size*len(titles),replace=True)\n",
    "        for i,t in enumerate(titles):\n",
    "            if np.random.random()>0.5:\n",
    "                sub_titles = sub_sampling_titles[i*self.sub_sampling_size:(i+1)*self.sub_sampling_size]\n",
    "                samples = sampling_based_on_similarity(t,sub_titles)\n",
    "                neg_titles[i*self.number_negative_tokens:(i+1)*self.number_negative_tokens]=samples\n",
    "        \n",
    "        neg_title_input_batch = [title_char_tokenizer(t,title_max_len) for t in neg_titles]\n",
    "        neg_title_input_batch = [neg_title_input_batch[x:x+self.number_negative_tokens]                                  for x in range(0, len(neg_title_input_batch), self.number_negative_tokens)]\n",
    "        \n",
    "        \n",
    "             \n",
    "        ## Convert inputs and output to numpy arrays\n",
    "        inputs = list(map(np.array,[desc_input_batch,title_input_batch,neg_title_input_batch]))\n",
    "        outputs = list(map(np.array,[y_pos_batch,y_neg_batch]))     \n",
    "        #return (inputs, outputs,titles,neg_titles) \n",
    "        #print(inputs)\n",
    "        return (inputs, outputs) \n",
    "    \n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        #'Shuffle data after each epoch'\n",
    "        np.random.shuffle(self.input_data)\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "batch_size=80\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "trainGen = empirical_sampling_gen(training_data,batch_size,number_of_negative_tokens,                                  sampling_titles_array,sampling_prob_array,sub_sampling_size=2200)                                 \n",
    "\n",
    "\n",
    "\n",
    "#len(title_char_to_index_dict)\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "\n",
    "testGen = empirical_sampling_gen(test_data,batch_size,number_of_negative_tokens,                                    sampling_titles_array,sampling_prob_array,sub_sampling_size=1500)\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "validateGen = empirical_sampling_gen(validation_data,batch_size,number_of_negative_tokens,                                    sampling_titles_array,sampling_prob_array,sub_sampling_size=2200)                                 \n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "target_title_input=Input(shape=(number_of_negative_tokens,title_max_len), dtype='int32', name='targe_title_input')\n",
    "neg_title_input = Input(shape=(number_of_negative_tokens,title_max_len), dtype='int32', name='ne_title_input')\n",
    "title_embedding_model = char_embed_model_seq(target_title_input,title_max_len,title_char_to_index_dict,\"titlee\")\n",
    "target_title_embed = title_embedding_model(target_title_input)\n",
    "target_title_embed = Lambda(lambda x:x[:,0,:],name='title_embedding_take_first_elemente')(target_title_embed)\n",
    "neg_title_embed = title_embedding_model(neg_title_input)\n",
    "\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "## Get resume job description embedding\n",
    "desc_input = Input(shape=(desc_max_length,),name='des_input')\n",
    "desc_embedding_model = desc_embed_model(desc_input,vocabSize+2,'des')\n",
    "desc_embed = desc_embedding_model(desc_input)\n",
    "\n",
    "## Calcuate dot product and loss\n",
    "desc_target_title_dot = keras.layers.dot([target_title_embed,desc_embed],axes=-1,name='des_target_dot')\n",
    "pos_title_logit = Activation('sigmoid',name='po_title_logit')(desc_target_title_dot)\n",
    "\n",
    "desc_neg_title_dot = keras.layers.dot([neg_title_embed,desc_embed],axes=-1,name='des_neg_dot')\n",
    "neg_title_logit = Activation('sigmoid',name='ne_title_logit')(desc_neg_title_dot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = desc_embedding_model.get_weights()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_plot(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "\n",
    "    for word in model.wv.vocab:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsne_pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_embeddings = {w:embeddings[idx].flatten() for w, idx in descTokenCounts.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "   #count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recipe_emb(temp, words_embeddings):\n",
    "    np.set_printoptions(suppress=True,\n",
    "    formatter={'float_kind':'{:0.60f}'.format})\n",
    "    count = 0\n",
    "    v=1\n",
    "    for w in temp.split():\n",
    "        if w in words_embeddings:\n",
    "            count *=1\n",
    "            v-= ((words_embeddings[w]))\n",
    "        #v= np.array(v)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.float_format', lambda x: '%.3f' % x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3= df[0:90000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jermainemarshall/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    }
   ],
   "source": [
    "df3['embed_summed']=df3['RECIPE'].apply(lambda x: recipe_emb(x,words_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RECIPE</th>\n",
       "      <th>WHO</th>\n",
       "      <th>embed_summed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mayonnaise 237.0 parmigianoreggianocheese 75.0 granulatedsugar 25.200000000000003 romainelettuce 283.0 baconbits 56.0</td>\n",
       "      <td>one</td>\n",
       "      <td>[1.2556392, 1.013125, 0.9881612, 1.0326676, 1.1220996, 0.92741096, 1.0696841, 0.95893633, 0.9399829, 0.85452765, 1.0248402, 0.8161403, 1.0942822, 0.9560012, 0.9015771, 0.8706607, 0.9667885, 1.1423523, 0.91212034, 1.0267096, 1.1208909, 0.96120334, 1.0034764, 0.95535743, 1.1576126, 0.9084617, 0.94192725, 0.9744645, 0.95841026, 1.0107601, 0.8751195, 0.98867327, 1.0371549, 1.0868069, 0.9068235, 1.0925081, 1.0925367, 0.80589217, 0.8248976, 0.88848144, 0.95271, 0.93624926, 0.93336254, 0.93286663, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PhiladelphiaCreamCheese 454.0 fineseasalt 3.0 pecanhalves 164.0</td>\n",
       "      <td>one</td>\n",
       "      <td>[0.9448704, 0.9506966, 0.9196438, 1.0018139, 0.989734, 1.0773984, 0.85884434, 1.0148004, 0.99326205, 0.9288898, 0.9527746, 0.9634197, 0.99564266, 1.148674, 1.0345888, 0.88012284, 1.0006186, 1.0065337, 1.0051363, 1.1366804, 0.9735658, 0.88369405, 0.97622985, 1.1346495, 1.0143901, 0.93725085, 0.937709, 0.9401871, 0.9497963, 1.0668398, 1.0237998, 1.0752032, 0.9698898, 1.0987651, 0.99184936, 0.9644332, 1.0457834, 0.8783677, 0.9586544, 0.936395, 1.1357902, 1.0035807, 0.93785346, 1.0506476, 0.9406...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hardboiledeggs 100.0 wholemilk 231.31199999999998 cakeflour 83.0 fineseasalt 0.4 vegetableoil 6.8</td>\n",
       "      <td>three</td>\n",
       "      <td>[1.1187569, 0.85408264, 1.1173044, 0.9470272, 1.0344192, 0.96887225, 1.0237195, 0.9795352, 0.9516653, 0.94139755, 1.0744181, 0.926491, 0.88205814, 0.8416496, 0.8900257, 1.1275212, 1.0840355, 1.1815677, 1.0186038, 1.0668372, 1.043772, 1.0040866, 1.0084603, 1.0069008, 1.15898, 1.1154802, 1.0714413, 0.8143189, 0.97373164, 0.82489383, 0.97472304, 0.87322104, 1.1389589, 0.9378641, 1.0277966, 1.1303847, 0.9410959, 0.94735867, 1.0459634, 0.9439875, 1.0121481, 0.98459184, 1.0575035, 0.8594607, 0.868...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                  RECIPE  \\\n",
       "0  mayonnaise 237.0 parmigianoreggianocheese 75.0 granulatedsugar 25.200000000000003 romainelettuce 283.0 baconbits 56.0   \n",
       "1                                                        PhiladelphiaCreamCheese 454.0 fineseasalt 3.0 pecanhalves 164.0   \n",
       "2                      hardboiledeggs 100.0 wholemilk 231.31199999999998 cakeflour 83.0 fineseasalt 0.4 vegetableoil 6.8   \n",
       "\n",
       "     WHO  \\\n",
       "0    one   \n",
       "1    one   \n",
       "2  three   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          embed_summed  \n",
       "0  [1.2556392, 1.013125, 0.9881612, 1.0326676, 1.1220996, 0.92741096, 1.0696841, 0.95893633, 0.9399829, 0.85452765, 1.0248402, 0.8161403, 1.0942822, 0.9560012, 0.9015771, 0.8706607, 0.9667885, 1.1423523, 0.91212034, 1.0267096, 1.1208909, 0.96120334, 1.0034764, 0.95535743, 1.1576126, 0.9084617, 0.94192725, 0.9744645, 0.95841026, 1.0107601, 0.8751195, 0.98867327, 1.0371549, 1.0868069, 0.9068235, 1.0925081, 1.0925367, 0.80589217, 0.8248976, 0.88848144, 0.95271, 0.93624926, 0.93336254, 0.93286663, ...  \n",
       "1  [0.9448704, 0.9506966, 0.9196438, 1.0018139, 0.989734, 1.0773984, 0.85884434, 1.0148004, 0.99326205, 0.9288898, 0.9527746, 0.9634197, 0.99564266, 1.148674, 1.0345888, 0.88012284, 1.0006186, 1.0065337, 1.0051363, 1.1366804, 0.9735658, 0.88369405, 0.97622985, 1.1346495, 1.0143901, 0.93725085, 0.937709, 0.9401871, 0.9497963, 1.0668398, 1.0237998, 1.0752032, 0.9698898, 1.0987651, 0.99184936, 0.9644332, 1.0457834, 0.8783677, 0.9586544, 0.936395, 1.1357902, 1.0035807, 0.93785346, 1.0506476, 0.9406...  \n",
       "2  [1.1187569, 0.85408264, 1.1173044, 0.9470272, 1.0344192, 0.96887225, 1.0237195, 0.9795352, 0.9516653, 0.94139755, 1.0744181, 0.926491, 0.88205814, 0.8416496, 0.8900257, 1.1275212, 1.0840355, 1.1815677, 1.0186038, 1.0668372, 1.043772, 1.0040866, 1.0084603, 1.0069008, 1.15898, 1.1154802, 1.0714413, 0.8143189, 0.97373164, 0.82489383, 0.97472304, 0.87322104, 1.1389589, 0.9378641, 1.0277966, 1.1303847, 0.9410959, 0.94735867, 1.0459634, 0.9439875, 1.0121481, 0.98459184, 1.0575035, 0.8594607, 0.868...  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jermainemarshall/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:4401: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self[name] = value\n"
     ]
    }
   ],
   "source": [
    "df3.embed_summed= df3['embed_summed'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jermainemarshall/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n",
      "/Users/jermainemarshall/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "/Users/jermainemarshall/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  after removing the cwd from sys.path.\n",
      "/Users/jermainemarshall/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"\n"
     ]
    }
   ],
   "source": [
    "df3.embed_summed.str.replace('[','')\n",
    "df3['embed_summed'] = df3['embed_summed'].str.replace('[', '')\n",
    "df3['embed_summed'] = df3['embed_summed'].str.replace(']', '')\n",
    "df3['embed_summed'] = df3['embed_summed'].str.replace('[\\n\\s]+', ',')\n",
    "df3['embed_summed'] = df3['embed_summed'].str.replace(r'^,', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "df4 = df3['embed_summed'].apply(lambda x: pd.Series(x.split(',')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3= df3[['RECIPE','WHO']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3= df3.join(df4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>RECIPE</th>\n",
       "      <th>WHO</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>mayonnaise 237.0 parmigianoreggianocheese 75.0 granulatedsugar 25.200000000000003 romainelettuce 283.0 baconbits 56.0</td>\n",
       "      <td>one</td>\n",
       "      <td>1.255639195442199707031250000000000000000000000000000000000000</td>\n",
       "      <td>1.013124942779541015625000000000000000000000000000000000000000</td>\n",
       "      <td>0.988161206245422363281250000000000000000000000000000000000000</td>\n",
       "      <td>1.032667636871337890625000000000000000000000000000000000000000</td>\n",
       "      <td>1.122099637985229492187500000000000000000000000000000000000000</td>\n",
       "      <td>0.927410960197448730468750000000000000000000000000000000000000</td>\n",
       "      <td>1.069684147834777832031250000000000000000000000000000000000000</td>\n",
       "      <td>0.958936333656311035156250000000000000000000000000000000000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.081253647804260253906250000000000000000000000000000000000000</td>\n",
       "      <td>0.841649353504180908203125000000000000000000000000000000000000</td>\n",
       "      <td>0.947954654693603515625000000000000000000000000000000000000000</td>\n",
       "      <td>0.986992895603179931640625000000000000000000000000000000000000</td>\n",
       "      <td>0.903862535953521728515625000000000000000000000000000000000000</td>\n",
       "      <td>0.959453523159027099609375000000000000000000000000000000000000</td>\n",
       "      <td>0.970603942871093750000000000000000000000000000000000000000000</td>\n",
       "      <td>1.094742178916931152343750000000000000000000000000000000000000</td>\n",
       "      <td>1.052498459815979003906250000000000000000000000000000000000000</td>\n",
       "      <td>0.956190824508666992187500000000000000000000000000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>PhiladelphiaCreamCheese 454.0 fineseasalt 3.0 pecanhalves 164.0</td>\n",
       "      <td>one</td>\n",
       "      <td>0.944870412349700927734375000000000000000000000000000000000000</td>\n",
       "      <td>0.950696587562561035156250000000000000000000000000000000000000</td>\n",
       "      <td>0.919643819332122802734375000000000000000000000000000000000000</td>\n",
       "      <td>1.001813888549804687500000000000000000000000000000000000000000</td>\n",
       "      <td>0.989733994007110595703125000000000000000000000000000000000000</td>\n",
       "      <td>1.077398419380187988281250000000000000000000000000000000000000</td>\n",
       "      <td>0.858844339847564697265625000000000000000000000000000000000000</td>\n",
       "      <td>1.014800429344177246093750000000000000000000000000000000000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.940294086933135986328125000000000000000000000000000000000000</td>\n",
       "      <td>1.006771922111511230468750000000000000000000000000000000000000</td>\n",
       "      <td>1.005477547645568847656250000000000000000000000000000000000000</td>\n",
       "      <td>1.095989704132080078125000000000000000000000000000000000000000</td>\n",
       "      <td>1.031980633735656738281250000000000000000000000000000000000000</td>\n",
       "      <td>1.040227890014648437500000000000000000000000000000000000000000</td>\n",
       "      <td>0.993172228336334228515625000000000000000000000000000000000000</td>\n",
       "      <td>1.010775566101074218750000000000000000000000000000000000000000</td>\n",
       "      <td>1.028016328811645507812500000000000000000000000000000000000000</td>\n",
       "      <td>1.062635064125061035156250000000000000000000000000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hardboiledeggs 100.0 wholemilk 231.31199999999998 cakeflour 83.0 fineseasalt 0.4 vegetableoil 6.8</td>\n",
       "      <td>three</td>\n",
       "      <td>1.118756890296936035156250000000000000000000000000000000000000</td>\n",
       "      <td>0.854082643985748291015625000000000000000000000000000000000000</td>\n",
       "      <td>1.117304444313049316406250000000000000000000000000000000000000</td>\n",
       "      <td>0.947027206420898437500000000000000000000000000000000000000000</td>\n",
       "      <td>1.034419178962707519531250000000000000000000000000000000000000</td>\n",
       "      <td>0.968872249126434326171875000000000000000000000000000000000000</td>\n",
       "      <td>1.023719549179077148437500000000000000000000000000000000000000</td>\n",
       "      <td>0.979535222053527832031250000000000000000000000000000000000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.130256772041320800781250000000000000000000000000000000000000</td>\n",
       "      <td>1.014510273933410644531250000000000000000000000000000000000000</td>\n",
       "      <td>1.174296975135803222656250000000000000000000000000000000000000</td>\n",
       "      <td>1.069943666458129882812500000000000000000000000000000000000000</td>\n",
       "      <td>1.010989069938659667968750000000000000000000000000000000000000</td>\n",
       "      <td>1.024985551834106445312500000000000000000000000000000000000000</td>\n",
       "      <td>1.066484451293945312500000000000000000000000000000000000000000</td>\n",
       "      <td>1.040072321891784667968750000000000000000000000000000000000000</td>\n",
       "      <td>0.947046458721160888671875000000000000000000000000000000000000</td>\n",
       "      <td>0.953204691410064697265625000000000000000000000000000000000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>zucchini 646.0 extra-virginoliveoil 13.5 hotwater 58.21333333333333 fineseasalt 0.4</td>\n",
       "      <td>three</td>\n",
       "      <td>1.123491168022155761718750000000000000000000000000000000000000</td>\n",
       "      <td>0.887899756431579589843750000000000000000000000000000000000000</td>\n",
       "      <td>0.998544991016387939453125000000000000000000000000000000000000</td>\n",
       "      <td>0.961351811885833740234375000000000000000000000000000000000000</td>\n",
       "      <td>1.103941917419433593750000000000000000000000000000000000000000</td>\n",
       "      <td>0.987994790077209472656250000000000000000000000000000000000000</td>\n",
       "      <td>1.031124472618103027343750000000000000000000000000000000000000</td>\n",
       "      <td>0.949861407279968261718750000000000000000000000000000000000000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.000710964202880859375000000000000000000000000000000000000000</td>\n",
       "      <td>1.119410395622253417968750000000000000000000000000000000000000</td>\n",
       "      <td>1.172864675521850585937500000000000000000000000000000000000000</td>\n",
       "      <td>1.069024801254272460937500000000000000000000000000000000000000</td>\n",
       "      <td>0.898526132106781005859375000000000000000000000000000000000000</td>\n",
       "      <td>1.183379888534545898437500000000000000000000000000000000000000</td>\n",
       "      <td>1.013421058654785156250000000000000000000000000000000000000000</td>\n",
       "      <td>0.960489511489868164062500000000000000000000000000000000000000</td>\n",
       "      <td>0.891258597373962402343750000000000000000000000000000000000000</td>\n",
       "      <td>0.866524755954742431640625000000000000000000000000000000000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 102 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                  RECIPE  \\\n",
       "0  mayonnaise 237.0 parmigianoreggianocheese 75.0 granulatedsugar 25.200000000000003 romainelettuce 283.0 baconbits 56.0   \n",
       "1                                                        PhiladelphiaCreamCheese 454.0 fineseasalt 3.0 pecanhalves 164.0   \n",
       "2                      hardboiledeggs 100.0 wholemilk 231.31199999999998 cakeflour 83.0 fineseasalt 0.4 vegetableoil 6.8   \n",
       "3                                    zucchini 646.0 extra-virginoliveoil 13.5 hotwater 58.21333333333333 fineseasalt 0.4   \n",
       "\n",
       "     WHO                                                               0  \\\n",
       "0    one  1.255639195442199707031250000000000000000000000000000000000000   \n",
       "1    one  0.944870412349700927734375000000000000000000000000000000000000   \n",
       "2  three  1.118756890296936035156250000000000000000000000000000000000000   \n",
       "3  three  1.123491168022155761718750000000000000000000000000000000000000   \n",
       "\n",
       "                                                                1  \\\n",
       "0  1.013124942779541015625000000000000000000000000000000000000000   \n",
       "1  0.950696587562561035156250000000000000000000000000000000000000   \n",
       "2  0.854082643985748291015625000000000000000000000000000000000000   \n",
       "3  0.887899756431579589843750000000000000000000000000000000000000   \n",
       "\n",
       "                                                                2  \\\n",
       "0  0.988161206245422363281250000000000000000000000000000000000000   \n",
       "1  0.919643819332122802734375000000000000000000000000000000000000   \n",
       "2  1.117304444313049316406250000000000000000000000000000000000000   \n",
       "3  0.998544991016387939453125000000000000000000000000000000000000   \n",
       "\n",
       "                                                                3  \\\n",
       "0  1.032667636871337890625000000000000000000000000000000000000000   \n",
       "1  1.001813888549804687500000000000000000000000000000000000000000   \n",
       "2  0.947027206420898437500000000000000000000000000000000000000000   \n",
       "3  0.961351811885833740234375000000000000000000000000000000000000   \n",
       "\n",
       "                                                                4  \\\n",
       "0  1.122099637985229492187500000000000000000000000000000000000000   \n",
       "1  0.989733994007110595703125000000000000000000000000000000000000   \n",
       "2  1.034419178962707519531250000000000000000000000000000000000000   \n",
       "3  1.103941917419433593750000000000000000000000000000000000000000   \n",
       "\n",
       "                                                                5  \\\n",
       "0  0.927410960197448730468750000000000000000000000000000000000000   \n",
       "1  1.077398419380187988281250000000000000000000000000000000000000   \n",
       "2  0.968872249126434326171875000000000000000000000000000000000000   \n",
       "3  0.987994790077209472656250000000000000000000000000000000000000   \n",
       "\n",
       "                                                                6  \\\n",
       "0  1.069684147834777832031250000000000000000000000000000000000000   \n",
       "1  0.858844339847564697265625000000000000000000000000000000000000   \n",
       "2  1.023719549179077148437500000000000000000000000000000000000000   \n",
       "3  1.031124472618103027343750000000000000000000000000000000000000   \n",
       "\n",
       "                                                                7  \\\n",
       "0  0.958936333656311035156250000000000000000000000000000000000000   \n",
       "1  1.014800429344177246093750000000000000000000000000000000000000   \n",
       "2  0.979535222053527832031250000000000000000000000000000000000000   \n",
       "3  0.949861407279968261718750000000000000000000000000000000000000   \n",
       "\n",
       "                                ...                                \\\n",
       "0                               ...                                 \n",
       "1                               ...                                 \n",
       "2                               ...                                 \n",
       "3                               ...                                 \n",
       "\n",
       "                                                               90  \\\n",
       "0  1.081253647804260253906250000000000000000000000000000000000000   \n",
       "1  0.940294086933135986328125000000000000000000000000000000000000   \n",
       "2  1.130256772041320800781250000000000000000000000000000000000000   \n",
       "3  1.000710964202880859375000000000000000000000000000000000000000   \n",
       "\n",
       "                                                               91  \\\n",
       "0  0.841649353504180908203125000000000000000000000000000000000000   \n",
       "1  1.006771922111511230468750000000000000000000000000000000000000   \n",
       "2  1.014510273933410644531250000000000000000000000000000000000000   \n",
       "3  1.119410395622253417968750000000000000000000000000000000000000   \n",
       "\n",
       "                                                               92  \\\n",
       "0  0.947954654693603515625000000000000000000000000000000000000000   \n",
       "1  1.005477547645568847656250000000000000000000000000000000000000   \n",
       "2  1.174296975135803222656250000000000000000000000000000000000000   \n",
       "3  1.172864675521850585937500000000000000000000000000000000000000   \n",
       "\n",
       "                                                               93  \\\n",
       "0  0.986992895603179931640625000000000000000000000000000000000000   \n",
       "1  1.095989704132080078125000000000000000000000000000000000000000   \n",
       "2  1.069943666458129882812500000000000000000000000000000000000000   \n",
       "3  1.069024801254272460937500000000000000000000000000000000000000   \n",
       "\n",
       "                                                               94  \\\n",
       "0  0.903862535953521728515625000000000000000000000000000000000000   \n",
       "1  1.031980633735656738281250000000000000000000000000000000000000   \n",
       "2  1.010989069938659667968750000000000000000000000000000000000000   \n",
       "3  0.898526132106781005859375000000000000000000000000000000000000   \n",
       "\n",
       "                                                               95  \\\n",
       "0  0.959453523159027099609375000000000000000000000000000000000000   \n",
       "1  1.040227890014648437500000000000000000000000000000000000000000   \n",
       "2  1.024985551834106445312500000000000000000000000000000000000000   \n",
       "3  1.183379888534545898437500000000000000000000000000000000000000   \n",
       "\n",
       "                                                               96  \\\n",
       "0  0.970603942871093750000000000000000000000000000000000000000000   \n",
       "1  0.993172228336334228515625000000000000000000000000000000000000   \n",
       "2  1.066484451293945312500000000000000000000000000000000000000000   \n",
       "3  1.013421058654785156250000000000000000000000000000000000000000   \n",
       "\n",
       "                                                               97  \\\n",
       "0  1.094742178916931152343750000000000000000000000000000000000000   \n",
       "1  1.010775566101074218750000000000000000000000000000000000000000   \n",
       "2  1.040072321891784667968750000000000000000000000000000000000000   \n",
       "3  0.960489511489868164062500000000000000000000000000000000000000   \n",
       "\n",
       "                                                               98  \\\n",
       "0  1.052498459815979003906250000000000000000000000000000000000000   \n",
       "1  1.028016328811645507812500000000000000000000000000000000000000   \n",
       "2  0.947046458721160888671875000000000000000000000000000000000000   \n",
       "3  0.891258597373962402343750000000000000000000000000000000000000   \n",
       "\n",
       "                                                               99  \n",
       "0  0.956190824508666992187500000000000000000000000000000000000000  \n",
       "1  1.062635064125061035156250000000000000000000000000000000000000  \n",
       "2  0.953204691410064697265625000000000000000000000000000000000000  \n",
       "3  0.866524755954742431640625000000000000000000000000000000000000  \n",
       "\n",
       "[4 rows x 102 columns]"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df3.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3.to_csv('Embeddings_files/subtraction_allrecipes_embeddings_with_counts.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mayonnaise\n",
      "parmigianoreggianocheese\n",
      "granulatedsugar\n",
      "romainelettuce\n",
      "baconbits\n"
     ]
    }
   ],
   "source": [
    "for w in df2['RECIPE'][0].split():\n",
    "    \n",
    "    print(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_embed_p(wordstr):\n",
    "    sum = 0\n",
    "    \n",
    "    if word in embedding_dict:\n",
    "        for i,word in embedding_dict.items():\n",
    "            sum+=i[word]\n",
    "    return sum\n",
    "            \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.00240331,  0.03762205, -0.02749192, -0.04848162,  0.04228977,\n",
       "       -0.01813643,  0.01563834, -0.03912672, -0.02333993,  0.0201702 ,\n",
       "       -0.03688854, -0.02473341, -0.01886275, -0.03389432, -0.00428972,\n",
       "        0.02012581,  0.04544756,  0.02659816, -0.00307486,  0.01334628,\n",
       "        0.00992028, -0.03446176,  0.03543515,  0.00474365, -0.02860992,\n",
       "       -0.04367108,  0.02054013,  0.00182164,  0.04817497,  0.02690083,\n",
       "        0.01221266,  0.02760795, -0.01784338,  0.02309927, -0.03757583,\n",
       "       -0.04131224, -0.00277395,  0.00288052, -0.01536344,  0.00434124,\n",
       "        0.04562828, -0.04436678, -0.03301162, -0.01961676,  0.02871481,\n",
       "       -0.04177602,  0.01416811, -0.03735606,  0.02146654, -0.01196771,\n",
       "       -0.00378642, -0.00040765,  0.03887788,  0.02748281,  0.02077584,\n",
       "        0.00117525,  0.03375361,  0.02505709, -0.04063374,  0.03086812,\n",
       "       -0.03723745,  0.00302555,  0.02797593, -0.00828804,  0.03461227,\n",
       "        0.00691745, -0.01064347,  0.03237707,  0.04385226,  0.00168307,\n",
       "       -0.02645704, -0.04941719, -0.04269782, -0.01728451,  0.00166322,\n",
       "       -0.00313818, -0.0401125 ,  0.03802038, -0.03888739,  0.01720229,\n",
       "       -0.0367116 ,  0.03285438,  0.00212562, -0.02917227,  0.0352801 ,\n",
       "        0.03068035,  0.01597795,  0.00494642, -0.03182699, -0.04325518,\n",
       "       -0.04414397,  0.04083182, -0.01446997,  0.0075372 , -0.03037521,\n",
       "       -0.01172737, -0.04149855, -0.0235227 , -0.01721592, -0.02402811],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words_embeddings['soup'].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 478,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model=Model(inputs=[desc_input,target_title_input,neg_title_input],outputs=[pos_title_logit,neg_title_logit])\n",
    "\n",
    "\n",
    "original_model.compile(optimizer='adam',metrics=['accuracy'],\n",
    "                       loss=['binary_crossentropy','binary_crossentropy'],loss_weights=[1.,1.])                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "30/30 [==============================] - 362s 12s/step - loss: 1.3836 - po_title_logit_loss: 0.6895 - ne_title_logit_loss: 0.6941 - po_title_logit_acc: 0.6250 - ne_title_logit_acc: 0.4503 - val_loss: 1.3797 - val_po_title_logit_loss: 0.6844 - val_ne_title_logit_loss: 0.6952 - val_po_title_logit_acc: 0.6983 - val_ne_title_logit_acc: 0.4325\n",
      "Epoch 2/5\n",
      "15/30 [==============>...............] - ETA: 53s - loss: 1.3794 - po_title_logit_loss: 0.6817 - ne_title_logit_loss: 0.6978 - po_title_logit_acc: 0.7408 - ne_title_logit_acc: 0.3737"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-479-22fd7a19a121>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m original_model.fit_generator(trainGen,verbose=1,validation_data=validateGen,epochs=5, steps_per_epoch= 30,\n\u001b[0;32m----> 2\u001b[0;31m                              workers=n_workers,max_queue_size=queue_size,use_multiprocessing=False,shuffle=True)\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1415\u001b[0;31m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1417\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[1;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[1;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 213\u001b[0;31m                                             class_weight=class_weight)\n\u001b[0m\u001b[1;32m    214\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1215\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1216\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2664\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2665\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2666\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2667\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2668\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2635\u001b[0m                                 session)\n\u001b[0;32m-> 2636\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2637\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2638\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "original_model.fit_generator(trainGen,verbose=1,validation_data=validateGen,epochs=5, steps_per_epoch= 30,\n",
    "                             workers=n_workers,max_queue_size=queue_size,use_multiprocessing=False,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "open('all_recipe_model.json'.format(model_name), 'w').write(original_model.to_json())\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "original_model.save_weights('all_recipe_model.hdf5'.format(name=model_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
