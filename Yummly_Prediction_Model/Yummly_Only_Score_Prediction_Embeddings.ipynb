{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jermainemarshall/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "embed_dim=100\n",
    "G=4\n",
    "#batch_size=80\n",
    "number_of_negative_tokens = 5\n",
    "n_epochs = 30\n",
    "n_workers=64\n",
    "queue_size = 64\n",
    "padding_length=3\n",
    "title_length_percentile=95\n",
    "desc_length_percentile=90\n",
    "\n",
    "import os, time, json,pandas as pd, numpy as np, re, string, random,sys\n",
    "from collections import Counter,defaultdict\n",
    "import Levenshtein\n",
    "\n",
    "\n",
    "from keras.layers import Input,Lambda,Conv1D,GlobalMaxPool1D,concatenate,Dense,Dropout,PReLU,TimeDistributed,Embedding,Activation\n",
    "from keras import Model\n",
    "import keras.backend as K\n",
    "import keras\n",
    "from keras.utils import Sequence\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "df= pd.read_csv('yummly.csv')\n",
    "\n",
    "\n",
    "# In[7]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    30405\n",
       "2    26269\n",
       "3    15468\n",
       "4     8259\n",
       "5     4050\n",
       "0     3357\n",
       "6     1413\n",
       "7      192\n",
       "Name: WHO, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.WHO.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip3 install python-levenshtein\n",
    "#!pip3 install setuptools\n",
    "\n",
    "# strip punctuation and combine ingredients into one word separated by a spae\n",
    "\n",
    "df['RECIPE'] = df['RECIPE'].str.replace('\\d+', '')\n",
    "df['RECIPE'] = df['RECIPE'].str.replace(' ', '')\n",
    "df['RECIPE'] = df['RECIPE'].str.replace('#.,', ' ')\n",
    "df['RECIPE'] = df['RECIPE'].str.replace('#.', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "who_scores= df.WHO.value_counts()\n",
    "plt.figure(figsize= (10,5))\n",
    "sns.barplot(who_scores.index, who_scores.values, alpha=0.8)\n",
    "plt.title('Distribution of WHO Scores')\n",
    "plt.ylabel('Number of Occurrences', fontsize=12)\n",
    "plt.xlabel('Health Score', fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['WHO'] = df['WHO'].replace({0:'zero', 1: 'one', 2: 'two', 3: 'three', 4: 'four', 5: 'five', 6: 'six', 7: 'seven'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth= 500\n",
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "#(X_train, X_test) = train_test_split(df2,test_size=0.20)\n",
    "\n",
    "\n",
    "#with tf.device('/gpu:0'):\n",
    "X_train, X_validation, X_test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
    "\n",
    "descTokenCounts = pd.Series(' '.join(df['RECIPE']).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descTokenCounts= descTokenCounts.value_counts().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "descTokenCounts.get('fineseasalt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "df= df.drop_duplicates()\n",
    "\n",
    "\n",
    "df= df.dropna()\n",
    "\n",
    "\n",
    "def remove_non_ascii_1(text):\n",
    "\n",
    "    return ''.join([i if ord(i) < 128 else ' ' for i in text])\n",
    "\n",
    "def remove_non_ascii_2(text):\n",
    "\n",
    "    return re.sub(r'[^\\x20-\\x7E]+',' ', text)\n",
    "\n",
    "vocabSize=20000\n",
    "tokens = sorted(descTokenCounts.items(),key=lambda x:x[1],reverse=True)[:vocabSize]\n",
    "\n",
    "tokens = [i[0] for i in tokens]\n",
    "tokens.append('token_unknown')\n",
    "token_to_index = {}\n",
    "index_to_token = {}\n",
    "\n",
    "\n",
    "df['WHO'] = ([remove_non_ascii_1(sentence) for sentence in df['WHO']])\n",
    "df['WHO'] = ([remove_non_ascii_2(sentence) for sentence in df['WHO']])\n",
    "\n",
    "\n",
    "df['RECIPE']= ([remove_non_ascii_1(sentence) for sentence in df['RECIPE']])\n",
    "\n",
    "\n",
    "# In[13]:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "df['RECIPE']= ([remove_non_ascii_2(sentence) for sentence in df['RECIPE']])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'allmy_jermaine_model1_jerm'\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "#(X_train, X_test) = train_test_split(df2,test_size=0.20)\n",
    "\n",
    "\n",
    "#with tf.device('/gpu:0'):\n",
    "X_train, X_validation, X_test = np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
    "\n",
    "\n",
    "\n",
    "with open('alltrain_data.json', 'wb') as f:\n",
    "    f.write(X_train.to_json(orient='records', lines=True).encode('ascii', 'ignore'))\n",
    "\n",
    "    \n",
    "with open('alltest_data.json', 'wb') as g:\n",
    "    g.write(X_test.to_json(orient='records', lines=True).encode('ascii', 'ignore'))\n",
    "\n",
    "\n",
    "    \n",
    "with open('allvalidation_data.json', 'wb') as t:\n",
    "    t.write(X_validation.to_json(orient='records', lines=True).encode('ascii', 'ignore'))\n",
    "\n",
    "def loadData(dataPath):\n",
    "    data=[]\n",
    "    start_time = time.time()\n",
    "    with open(dataPath) as f:\n",
    "        for line in f:\n",
    "            data.append(line)\n",
    "    loadTime = time.time() - start_time\n",
    "\n",
    "    #print loadTime\n",
    "    return data\n",
    "\n",
    "training_data = loadData('alltrain_data.json')\n",
    "validation_data = loadData('allvalidation_data.json') \n",
    "test_data = loadData('alltest_data.json') \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "title_length= df['WHO'].map(len)\n",
    "desc_length= df['RECIPE'].map(len)\n",
    "titleCounts = df['WHO'].value_counts().to_dict()\n",
    "\n",
    "charset=set(list(string.ascii_lowercase)+list(map(str,range(10)))+list(\"?$*!: @ ^ %- / <> ~ ` \\\\ [] {} \\n = | \\\" _+#&-/\\n %() ' '.;,\"))\n",
    "title_char_to_index_dict={c:i for i,c in enumerate(charset)}\n",
    "title_index_to_char_dict={i:c for c,i in title_char_to_index_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(title_char_to_index_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "for i in range(len(tokens)):\n",
    "    token_to_index[tokens[i]] = i+1\n",
    "    index_to_token[i+1] = tokens[i] \n",
    "\n",
    "\n",
    "with open('allmodels_indices.json'.format(model_name),'w') as f:\n",
    "    json.dump({'title_char_to_index_dict':title_char_to_index_dict,'title_index_to_char_dict':title_index_to_char_dict,              'token_to_index':token_to_index,'index_to_token':index_to_token},f)\n",
    "\n",
    "\n",
    "    \n",
    "title_max_len = np.ceil(np.percentile(title_length, title_length_percentile)).astype(int)\n",
    "\n",
    "def title_char_tokenizer(entity,entity_max_len):\n",
    "    preproced_entity=(\" \"*padding_length+entity)[:entity_max_len]\n",
    "    padded_entity=preproced_entity+\"\".join([\" \"]*(entity_max_len-len(preproced_entity)))\n",
    "    return [title_char_to_index_dict[i] for i in padded_entity]\n",
    "\n",
    "\n",
    "## RECIPE tokenizer\n",
    "## 0 padded in the end\n",
    "desc_max_length = np.ceil(np.percentile(desc_length, desc_length_percentile)).astype(int)\n",
    "def job_description_tokenizer(desc,entity_max_len):\n",
    "    desc = re.sub(\"[,\\/]\",' ',desc).split()\n",
    "    desc = [token_to_index[i] if i in token_to_index else token_to_index['token_unknown'] for i in desc]\n",
    "    desc=(desc+[0]*entity_max_len)[:entity_max_len]\n",
    "    return desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open ('allallmodels_model_params.json'.format(model_name),'w') as f:\n",
    "    json.dump({'title_max_len':int(title_max_len),'desc_max_length':int(desc_max_length)},f)\n",
    "    \n",
    "## character embedding model for a list of titles\n",
    "def char_embed_model_seq(input_layer,char_length,char_index_dict,entity_name,n_filters=512,embed_dim=embed_dim):\n",
    "    '''\n",
    "    Same as char_embed_model, but produces embeddings for a sequence of say, titles.\n",
    "    Applies the same layers (as char_embed_model) to each member of the sequence\n",
    "    '''    \n",
    "    entity_onehot_layer=TimeDistributed(Lambda(lambda x: K.one_hot(x,len(char_index_dict)),output_shape=(char_length,len(char_index_dict))),name=entity_name+'_onehot_seq')(input_layer)\n",
    "        \n",
    "    conv7_layer=TimeDistributed(Conv1D(filters=n_filters,kernel_size=4,activation='relu'),name=entity_name+'_conv_7seq')(entity_onehot_layer)\n",
    "    maxpool7_layer=TimeDistributed(GlobalMaxPool1D(),name=entity_name+'maxpool1seq')(conv7_layer)\n",
    "\n",
    "    conv3_layer=TimeDistributed(Conv1D(filters=n_filters,kernel_size=2,activation='relu'),name=entity_name+'_conv_3seq')(entity_onehot_layer)\n",
    "    maxpool3_layer=TimeDistributed(GlobalMaxPool1D(),name=entity_name+'maxpool2seq')(conv3_layer)\n",
    "\n",
    "    pools_concat=keras.layers.concatenate([maxpool7_layer,maxpool3_layer],name=entity_name+'_maxpools_concatseq')\n",
    "    #pools_dropped=(pools_concat)\n",
    "\n",
    "    entity_dense_layer=TimeDistributed(Dense(n_filters*2,activation='relu'),name=entity_name+'_intermediate_denseseq')(pools_concat)\n",
    "    \n",
    "    dropout_layer=TimeDistributed(Dropout(0.4),name=entity_name+\"_droppedseq\")(entity_dense_layer)\n",
    "    \n",
    "    entity_final_dense=TimeDistributed(Dense(embed_dim),name=entity_name+'_last_denseseq')(dropout_layer)\n",
    "    entity_prelu_layer=TimeDistributed(PReLU(),name=entity_name+'_preluseq')(entity_final_dense)\n",
    "    embed_norm_layer=TimeDistributed(Lambda(lambda x: x/(K.epsilon()+K.sqrt(K.batch_dot(x,x,axes=-1)))),name=entity_name+'_unit_norm_embedseq')(entity_prelu_layer)\n",
    "\n",
    "    entity_embed_model=keras.models.Model(input_layer,embed_norm_layer,name=entity_name+\"_embedding_model\")\n",
    "    return entity_embed_model\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# In[25]:\n",
    "\n",
    "\n",
    "## job describtion embedding - average word embedding (remove 0 padding)\n",
    "def desc_embed_model(input_layer,vocab_size,entity_name,embed_dim=embed_dim):\n",
    "    wordEmbed = Embedding(input_dim = vocab_size,output_dim = embed_dim, name='word_embedding')(input_layer) \n",
    "    \n",
    "    ## get average embedding for desc (remove 0 padding)\n",
    "    mask=Lambda(lambda x: K.cast(K.minimum(1,K.cast(x,'int32')),'float32'),name='mask')(input_layer)\n",
    "    mask_summed=Lambda(lambda x: 1/(K.epsilon()+K.sum(x,axis=-1)),name='mask_summed')(mask)\n",
    "    mask_normed=keras.layers.multiply([mask,mask_summed],name='mask_normed')\n",
    "    desc_embedding_averaged=keras.layers.dot([wordEmbed,mask_normed],axes=1,name='desc_embedding_averaged')\n",
    "    \n",
    "    desc_embed_model=Model(input_layer,desc_embedding_averaged,name=entity_name+\"_embedding_model\")\n",
    "    return desc_embed_model\n",
    "# In[26]:\n",
    "\n",
    "\n",
    "alpha=0.5\n",
    "sampling_titles = titleCounts.keys()\n",
    "\n",
    "sampling_titles= [k for k in titleCounts.keys()]\n",
    "\n",
    "\n",
    "# In[27]:\n",
    "\n",
    "\n",
    "sampling_titles_array = np.array(sampling_titles)\n",
    "\n",
    "\n",
    "# In[28]:\n",
    "\n",
    "\n",
    "sampling_prob_array = np.array(list(titleCounts.values()))\n",
    "sampling_prob_array = np.power(sampling_prob_array,alpha)\n",
    "sampling_prob_array = sampling_prob_array/sum(sampling_prob_array)\n",
    "\n",
    "\n",
    "# In[29]:\n",
    "\n",
    "\n",
    "def sampling_based_on_similarity(title,sub_sampling_titles,number_of_negative_tokens=5):\n",
    "## Randomly sample sub_sampling_titles based on emperical distribution\n",
    "## Randomly sample titles from the selected sub-titles based on Levenshtein similarity\n",
    "    st = time.time()\n",
    "    sampling_titles = np.array(sub_sampling_titles)\n",
    "\n",
    "    sampling_prob = np.array([Levenshtein.ratio(title,i) for i in sub_sampling_titles])#Levenshtein.ratio(title,i) for i in sub_sampling_titles])\n",
    "    sampling_prob = sampling_prob/sum(sampling_prob)\n",
    "    \n",
    "    samples=np.random.choice(sampling_titles,size=number_of_negative_tokens,p=sampling_prob)\n",
    "  \n",
    "    return samples.tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class empirical_sampling_gen(Sequence):\n",
    "\n",
    "    def __init__(self, data, batch_size,number_of_negative_tokens,sampling_titles,sampling_prob,sub_sampling_size):\n",
    "        '''        \n",
    "        batch_size: number of resumes to process per batch        \n",
    "        The actual number of training samples varies as each resume is splitted into sub-sequences\n",
    "        '''\n",
    "        self.batch_size = batch_size\n",
    "        self.input_data = data\n",
    "        self.data_length = len(data)\n",
    "        self.number_negative_tokens = number_of_negative_tokens\n",
    "        self.sampling_titles = sampling_titles\n",
    "        self.sampling_prob = sampling_prob\n",
    "        self.sub_sampling_size = sub_sampling_size\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.data_length / float(self.batch_size)))\n",
    "    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i = idx\n",
    "        batch_size = self.batch_size \n",
    "        desc_input_batch=[]\n",
    "        title_input_batch=[]\n",
    "        y_pos_batch=[]\n",
    "        y_neg_batch=[] \n",
    "        titles = []\n",
    "        for k in range(i * batch_size, min((i + 1) * batch_size, self.data_length)):\n",
    "            line = json.loads(self.input_data[k])\n",
    "            desc = job_description_tokenizer(line['RECIPE'],desc_max_length)\n",
    "            #desc = str(desc)\n",
    "            titles.append(line['WHO'])\n",
    "            title = title_char_tokenizer(line['WHO'],title_max_len)\n",
    "            #title= str(title)\n",
    "            desc_input_batch.append(desc)\n",
    "            title_input_batch.append([title]*self.number_negative_tokens)\n",
    "            y_pos_batch.append(1)\n",
    "            y_neg_batch.append([0]*self.number_negative_tokens)  \n",
    "            \n",
    "        ## Sample negative titles for each batch all together              \n",
    "        neg_titles = np.random.choice(self.sampling_titles, size=len(titles)*self.number_negative_tokens,p=self.sampling_prob)\n",
    "        \n",
    "        ## Resample for titles with length below median title length based on Levenshtein similarity\n",
    "        sub_sampling_titles = np.random.choice(self.sampling_titles,p=self.sampling_prob,                                               size=self.sub_sampling_size*len(titles),replace=True)\n",
    "        for i,t in enumerate(titles):\n",
    "            if np.random.random()>0.5:\n",
    "                sub_titles = sub_sampling_titles[i*self.sub_sampling_size:(i+1)*self.sub_sampling_size]\n",
    "                samples = sampling_based_on_similarity(t,sub_titles)\n",
    "                neg_titles[i*self.number_negative_tokens:(i+1)*self.number_negative_tokens]=samples\n",
    "        \n",
    "        neg_title_input_batch = [title_char_tokenizer(t,title_max_len) for t in neg_titles]\n",
    "        neg_title_input_batch = [neg_title_input_batch[x:x+self.number_negative_tokens]                                  for x in range(0, len(neg_title_input_batch), self.number_negative_tokens)]\n",
    "        \n",
    "        \n",
    "             \n",
    "        ## Convert inputs and output to numpy arrays\n",
    "        inputs = list(map(np.array,[desc_input_batch,title_input_batch,neg_title_input_batch]))\n",
    "        outputs = list(map(np.array,[y_pos_batch,y_neg_batch]))     \n",
    "        #return (inputs, outputs,titles,neg_titles) \n",
    "        #print(inputs)\n",
    "        return (inputs, outputs) \n",
    "    \n",
    "\n",
    "    def on_epoch_end(self):\n",
    "        #'Shuffle data after each epoch'\n",
    "        np.random.shuffle(self.input_data)\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "# In[31]:\n",
    "\n",
    "\n",
    "batch_size=100\n",
    "\n",
    "\n",
    "# In[32]:\n",
    "\n",
    "\n",
    "trainGen = empirical_sampling_gen(training_data,batch_size,number_of_negative_tokens,                                  sampling_titles_array,sampling_prob_array,sub_sampling_size=2200)                                 \n",
    "\n",
    "\n",
    "\n",
    "#len(title_char_to_index_dict)\n",
    "\n",
    "\n",
    "# In[33]:\n",
    "\n",
    "\n",
    "\n",
    "testGen = empirical_sampling_gen(test_data,batch_size,number_of_negative_tokens,                                    sampling_titles_array,sampling_prob_array,sub_sampling_size=1500)\n",
    "\n",
    "\n",
    "# In[34]:\n",
    "\n",
    "\n",
    "validateGen = empirical_sampling_gen(validation_data,batch_size,number_of_negative_tokens,                                    sampling_titles_array,sampling_prob_array,sub_sampling_size=2200)                                 \n",
    "\n",
    "\n",
    "# In[35]:\n",
    "\n",
    "\n",
    "target_title_input=Input(shape=(number_of_negative_tokens,title_max_len), dtype='int32', name='targe_title_input')\n",
    "neg_title_input = Input(shape=(number_of_negative_tokens,title_max_len), dtype='int32', name='ne_title_input')\n",
    "title_embedding_model = char_embed_model_seq(target_title_input,title_max_len,title_char_to_index_dict,\"titlee\")\n",
    "target_title_embed = title_embedding_model(target_title_input)\n",
    "target_title_embed = Lambda(lambda x:x[:,0,:],name='title_embedding_take_first_elemente')(target_title_embed)\n",
    "neg_title_embed = title_embedding_model(neg_title_input)\n",
    "\n",
    "\n",
    "\n",
    "# In[36]:\n",
    "\n",
    "\n",
    "## Get resume job description embedding\n",
    "desc_input = Input(shape=(desc_max_length,),name='des_input')\n",
    "desc_embedding_model = desc_embed_model(desc_input,vocabSize+2,'des')\n",
    "desc_embed = desc_embedding_model(desc_input)\n",
    "\n",
    "## Calcuate dot product and loss\n",
    "desc_target_title_dot = keras.layers.dot([target_title_embed,desc_embed],axes=-1,name='des_target_dot')\n",
    "pos_title_logit = Activation('sigmoid',name='po_title_logit')(desc_target_title_dot)\n",
    "\n",
    "desc_neg_title_dot = keras.layers.dot([neg_title_embed,desc_embed],axes=-1,name='des_neg_dot')\n",
    "neg_title_logit = Activation('sigmoid',name='ne_title_logit')(desc_neg_title_dot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "original_model=Model(inputs=[desc_input,target_title_input,neg_title_input],outputs=[pos_title_logit,neg_title_logit])\n",
    "\n",
    "\n",
    "original_model.compile(optimizer='adam',metrics=['accuracy'],\n",
    "                       loss=['binary_crossentropy','binary_crossentropy'],loss_weights=[1.,1.])                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "\n",
    "(plot_model(original_model, show_shapes= True, to_file='model.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model.fit_generator(trainGen,verbose=1,validation_data=validateGen,epochs=10, steps_per_epoch = 160,\n",
    "                             workers=n_workers,max_queue_size=queue_size,use_multiprocessing=False,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "open('recipe_yummly_model.json'.format(model_name), 'w').write(original_model.to_json())\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "original_model.save_weights('recipe_yummly_model.hdf5'.format(name=model_name))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "plt.subplot(212) \n",
    "plt.plot(original_model.history['neg_rec_loss']) \n",
    "plt.plot(original_model.history['val_neg_rec_loss']) \n",
    "plt.plot(original_model.history['pos_rec_loss'])  \n",
    "plt.plot(original_model.history['val_pos_rec_loss'])  \n",
    "plt.title('model loss')  \n",
    "plt.ylabel('loss')  \n",
    "plt.xlabel('epoch')  \n",
    "plt.savefig('alltloss')\n",
    "plt.legend(['neg train', 'neg val', 'pos train', 'pos val'], loc='upper left')  \n",
    "plt.show()\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#plt.figure(1)\\n\",\n",
    "plt.subplot(211) \n",
    "plt.plot(history.history['neg_rec_acc']) \n",
    "plt.plot(history.history['val_neg_rec_acc'])  \n",
    "plt.plot(history.history['pos_recipe_acc'])  \n",
    "plt.plot(history.history['val_pos_recipe_acc'])\n",
    "plt.title('model accuracy')  \n",
    "plt.ylabel('accuracy')  \n",
    "plt.xlabel('epoch')\n",
    "#plt.ylim(0,0.7)\n",
    "plt.savefig('allstacc')\n",
    "plt.legend(['neg t.acc', 'neg v.acc', 'pos t.acc', 'pos v.acc', 'upper left'])\n",
    "\n",
    "\n",
    "# In[ ]:\n",
    "\n",
    "\n",
    "#import matplotlib.pyplot as plt\n",
    "#get_ipython().run_line_magic('matplotlib', 'inline')\n",
    "#plt.subplot(212) \n",
    "#plt.plot(history.history['neg_title_logit_loss']) \n",
    "#plt.plot(history.history['val_neg_title_logit_loss']) \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
